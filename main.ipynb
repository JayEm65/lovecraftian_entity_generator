{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "\n",
    "# Set up necessary directories and configurations:\n",
    "os.makedirs('data', exist_ok=True)\n",
    "session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Clean title by standardizing the \"By H. P. Lovecraft\" text:\n",
    "def clean_title(title):\n",
    "    author_text = \"By H. P. Lovecraft\"\n",
    "    title = re.sub(rf\"({author_text}\\s*)+\", author_text, title).strip()\n",
    "    if title.endswith(author_text) and not title.endswith(\" \" + author_text):\n",
    "        title = title.replace(author_text, \" \" + author_text)\n",
    "    return title\n",
    "\n",
    "# --- Step 1: Scraping Lovecraft Works ---\n",
    "\n",
    "def scrape_lovecraft_content(content_type):\n",
    "    base_url = \"https://www.hplovecraft.com/writings/texts/\"\n",
    "    response = session.get(base_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access the base URL: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content_links = [\n",
    "        f\"{base_url}{link['href']}\"\n",
    "        for link in soup.find_all('a', href=True)\n",
    "        if link['href'].startswith(f'{content_type}/') and not link['href'].startswith('#')\n",
    "    ]\n",
    "\n",
    "    csv_filename = f'data/lovecraft_{content_type}.csv'\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Content Type', 'Title', 'Text'])\n",
    "\n",
    "        for content_url in content_links:\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            try:\n",
    "                content_response = session.get(content_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                if content_response.status_code == 200:\n",
    "                    content_soup = BeautifulSoup(content_response.content, 'html.parser')\n",
    "                    title_tag = content_soup.find('font', size=\"+2\")\n",
    "                    author_tag = content_soup.find('font', size=\"+1\")\n",
    "                    text_div = content_soup.find('div', align='justify')\n",
    "\n",
    "                    if title_tag and text_div:\n",
    "                        title = f\"{title_tag.get_text(strip=True)} by {author_tag.get_text(strip=True)}\"\n",
    "                        title = clean_title(title)  # Clean the title text\n",
    "                        csvwriter.writerow([content_type, title, text_div.get_text(strip=True)])\n",
    "                        print(f'Scraped: {title}')\n",
    "                    else:\n",
    "                        print(f'Title or text not found for {content_url}')\n",
    "                else:\n",
    "                    print(f'Failed to scrape {content_url}: {content_response.status_code}')\n",
    "            except Exception as e:\n",
    "                print(f'Error scraping {content_url}: {e}')\n",
    "\n",
    "# Scrape all specified content types:\n",
    "for content in ['fiction', 'poetry', 'essays', 'letters']:\n",
    "    scrape_lovecraft_content(content)\n",
    "\n",
    "# --- Step 2: API Data Collection ---\n",
    "\n",
    "def fetch_and_save_json(api_url, filename):\n",
    "    response = session.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(f'data/{filename}.json', 'w', encoding='utf-8') as file:\n",
    "            json.dump(response.json(), file, ensure_ascii=False, indent=4)\n",
    "        print(f\"{filename.capitalize()} data saved successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {filename} data: {response.status_code}\")\n",
    "\n",
    "api_categories = {\n",
    "    \"creatures\": \"https://lovecraftapirest.fly.dev/api/creatures\",\n",
    "    \"races\": \"https://lovecraftapirest.fly.dev/api/races\",\n",
    "    \"outer_gods\": \"https://lovecraftapirest.fly.dev/api/categories/outer-gods\",\n",
    "    \"great_old_ones\": \"https://lovecraftapirest.fly.dev/api/categories/great-old-ones\",\n",
    "    \"lesser_old_ones\": \"https://lovecraftapirest.fly.dev/api/categories/lesser-old-ones\"\n",
    "}\n",
    "\n",
    "for name, url in api_categories.items():\n",
    "    fetch_and_save_json(url, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Step 3: Load and Clean Data ---\n",
    "# Load JSON files containing Lovecraftian entities:\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"Load JSON data from a given file path.\"\"\"\n",
    "    with open(file_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "creatures = load_json_data('data/creatures.json')\n",
    "great_old_ones = load_json_data('data/great_old_ones.json')\n",
    "lesser_old_ones = load_json_data('data/lesser_old_ones.json')\n",
    "outer_gods = load_json_data('data/outer_gods.json')\n",
    "races = load_json_data('data/races.json')\n",
    "\n",
    "# Load Lovecraft fiction CSV data:\n",
    "lovecraft_data = pd.read_csv('data/lovecraft_fiction.csv')\n",
    "\n",
    "# --- Step 4: Combine Entity Data ---\n",
    "# Create list of all entities with their type for easy tracking:\n",
    "def create_entity_list(creatures, great_old_ones, lesser_old_ones, outer_gods, races):\n",
    "    \"\"\"Combine all Lovecraftian entities into a single list with their types.\"\"\"\n",
    "    all_entities = []\n",
    "    all_entities.extend([{'name': creature['name'], 'type': 'Creature'} for creature in creatures])\n",
    "    all_entities.extend([{'name': old_one['name'], 'type': 'Great Old One'} for old_one in great_old_ones])\n",
    "    all_entities.extend([{'name': old_one['name'], 'type': 'Lesser Old One'} for old_one in lesser_old_ones])\n",
    "    all_entities.extend([{'name': old_one['name'], 'type': 'Outer God'} for old_one in outer_gods])\n",
    "    all_entities.extend([{'name': race['race'], 'type': 'Race'} for race in races])\n",
    "    return all_entities\n",
    "\n",
    "all_entities = create_entity_list(creatures, great_old_ones, lesser_old_ones, outer_gods, races)\n",
    "\n",
    "# --- Step 5: Clean Entity Names ---\n",
    "# Normalize the entity names and merge duplicates:\n",
    "def clean_name(name):\n",
    "    \"\"\"Clean and standardize entity names to merge duplicates.\"\"\"\n",
    "    name_corrections = {\n",
    "        'ghoul': 'ghoul',\n",
    "        'dark young': 'dark young of shub-niggurath',\n",
    "        'deep one': 'deep one',\n",
    "        'dimensional shambler': 'dimensional shambler',\n",
    "        'elder thing': 'elder thing',\n",
    "        'flying polyp': 'flying polyp',\n",
    "        'ghast': 'ghast',\n",
    "        'gug': 'gug',\n",
    "        'hound of tindalos': 'hound of tindalos',\n",
    "        'moon-beast': 'moon-beast',\n",
    "        'night-gaunt': 'night-gaunt',\n",
    "        'shoggoth': 'shoggoth',\n",
    "        'spider of leng': 'spider of leng',\n",
    "        'star spawn of cthulhu': 'star spawn of cthulhu',\n",
    "        'tcho-tcho': 'tcho-tcho'\n",
    "    }\n",
    "    for singular, corrected in name_corrections.items():\n",
    "        if singular in name:\n",
    "            return corrected\n",
    "    return name\n",
    "\n",
    "# Apply cleaning function to entity names:\n",
    "all_names = [clean_name(entity['name'].lower()) for entity in all_entities]\n",
    "\n",
    "# Remove unwanted terms from the list:\n",
    "unwanted_terms = ['han', 'ghoul', 'darkness']\n",
    "all_names = [name for name in all_names if name not in unwanted_terms]\n",
    "\n",
    "# --- Step 6: Filter Texts by Entities ---\n",
    "def filter_texts(data_frame, names):\n",
    "    \"\"\"Filter Lovecraft texts based on a list of entity names.\"\"\"\n",
    "    return [text for text in data_frame['Text'] if any(name in text.lower() for name in names)]\n",
    "\n",
    "filtered_texts = filter_texts(lovecraft_data, all_names)\n",
    "\n",
    "# --- Step 7: Count Entity Occurrences ---\n",
    "# Count how often each entity appears in the filtered texts:\n",
    "name_counts = Counter()\n",
    "for text in filtered_texts:\n",
    "    for name in all_names:\n",
    "        name_counts[name] += text.lower().count(name)\n",
    "\n",
    "# Create a DataFrame to store name counts:\n",
    "name_counts_df = pd.DataFrame(name_counts.items(), columns=['Name', 'Count'])\n",
    "\n",
    "# Add 'Type' column to identify each entity's type:\n",
    "name_counts_df['Type'] = name_counts_df['Name'].apply(\n",
    "    lambda name: next((entity['type'] for entity in all_entities if entity['name'].lower() == name), 'Unknown')\n",
    ")\n",
    "\n",
    "# Save the name counts and types to a CSV file:\n",
    "name_counts_df.to_csv('data/lovecraft_name_counts.csv', index=False)\n",
    "\n",
    "# --- Step 8: Basic Visualization ---\n",
    "def plot_top_entities(counts_df, top_n=10, exclude=None):\n",
    "    \"\"\"Plot a bar chart of the top N entities by occurrence count, excluding specified terms.\"\"\"\n",
    "    exclude = exclude or []\n",
    "    filtered_df = counts_df[~counts_df['Name'].isin(exclude)]\n",
    "    top_entities = filtered_df.nlargest(top_n, 'Count')\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(top_entities['Name'], top_entities['Count'], color='purple')\n",
    "    plt.title('Top Lovecraftian Entities by Count')\n",
    "    plt.xlabel('Entity Names')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the top 20 entities, excluding 'darkness':\n",
    "plot_top_entities(name_counts_df, top_n=20, exclude=['darkness'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Load Data ---\n",
    "def load_name_counts(file_path):\n",
    "    \"\"\"Load Lovecraft name counts from a CSV file.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "name_counts_df = load_name_counts('data/lovecraft_name_counts.csv')\n",
    "\n",
    "# --- Step 2: Show Most and Least Common Entities ---\n",
    "def show_top_and_least_common_entities(counts_df, top_n=10):\n",
    "    \"\"\"\n",
    "    Display the top and least common entities based on their occurrence counts.\n",
    "    \n",
    "    Args:\n",
    "        counts_df (DataFrame): DataFrame containing entity name counts.\n",
    "        top_n (int): Number of top and least common entities to display.\n",
    "    \"\"\"\n",
    "    # Top N most common entities:\n",
    "    top_entities = counts_df.sort_values(by='Count', ascending=False).head(top_n)\n",
    "    print(\"Top Entities:\")\n",
    "    print(top_entities)\n",
    "    \n",
    "    # Top N least common entities:\n",
    "    least_common_entities = counts_df.sort_values(by='Count', ascending=True).head(top_n)\n",
    "    print(\"\\nLeast Common Entities:\")\n",
    "    print(least_common_entities)\n",
    "\n",
    "# Show the top 10 and least common 10 entities:\n",
    "show_top_and_least_common_entities(name_counts_df, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# --- Step 1: Load and Preprocess Data ---\n",
    "def load_and_prepare_data(file_path, seq_length=5):\n",
    "    \"\"\"\n",
    "    Load the dataset, clean and tokenize names, and prepare input-output sequences.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing entity names.\n",
    "        seq_length (int): Length of character sequences used to predict the next character.\n",
    "        \n",
    "    Returns:\n",
    "        X (np.array): Input data for the LSTM model.\n",
    "        y (np.array): One-hot encoded target labels.\n",
    "        char_to_idx (dict): Mapping of characters to indices.\n",
    "        idx_to_char (dict): Reverse mapping of indices to characters.\n",
    "    \"\"\"\n",
    "    # Load CSV and extract 'Name' column:\n",
    "    df = pd.read_csv(file_path)\n",
    "    names = df['Name'].tolist()\n",
    "\n",
    "    # Clean names by removing spaces and prepare vocab:\n",
    "    names = [name.replace(\" \", \"\") for name in names]\n",
    "    all_chars = ''.join(names)\n",
    "    vocab = sorted(set(all_chars))\n",
    "    \n",
    "    char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "    # Generate input-output sequences:\n",
    "    X, y = [], []\n",
    "    for name in names:\n",
    "        for i in range(len(name) - seq_length):\n",
    "            X.append([char_to_idx[char] for char in name[i:i + seq_length]])\n",
    "            y.append(char_to_idx[name[i + seq_length]])\n",
    "\n",
    "    # Convert to numpy arrays and normalize:\n",
    "    X = np.array(X).reshape((len(X), seq_length, 1)) / len(vocab)\n",
    "    y = tf.keras.utils.to_categorical(np.array(y), num_classes=len(vocab))\n",
    "    \n",
    "    return X, y, char_to_idx, idx_to_char\n",
    "\n",
    "# Load and prepare the data:\n",
    "X, y, char_to_idx, idx_to_char = load_and_prepare_data('data/lovecraft_name_counts.csv')\n",
    "\n",
    "# --- Step 2: Build and Compile LSTM Model ---\n",
    "def build_lstm_model(input_shape, vocab_size, lstm_units=128, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Build and compile an LSTM model for character-level text generation.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data.\n",
    "        vocab_size (int): Size of the vocabulary (number of unique characters).\n",
    "        lstm_units (int): Number of units in the LSTM layers.\n",
    "        dropout_rate (float): Dropout rate to prevent overfitting.\n",
    "        \n",
    "    Returns:\n",
    "        model (tf.keras.Model): Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, input_shape=input_shape, return_sequences=True),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build model:\n",
    "model = build_lstm_model(X.shape[1:], len(char_to_idx))\n",
    "\n",
    "# --- Step 3: Define Custom Callback ---\n",
    "class HighestAccuracyCallback(Callback):\n",
    "    \"\"\"\n",
    "    Custom callback to track and display the highest accuracy during training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.highest_accuracy = 0.0\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_accuracy = logs.get(\"accuracy\")\n",
    "        if current_accuracy and current_accuracy > self.highest_accuracy:\n",
    "            self.highest_accuracy = current_accuracy\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(f\"Highest accuracy achieved: {self.highest_accuracy:.4f}\")\n",
    "\n",
    "# Instantiate callback:\n",
    "highest_accuracy_callback = HighestAccuracyCallback()\n",
    "\n",
    "# --- Step 4: Train Model ---\n",
    "model.fit(X, y, epochs=1000, batch_size=64, callbacks=[highest_accuracy_callback])\n",
    "\n",
    "# --- Step 5: Generate Entity Names ---\n",
    "def generate_entity_name(length=10, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate a random entity name using the trained LSTM model with a temperature sampling method.\n",
    "    \n",
    "    Args:\n",
    "        length (int): Length of the generated name.\n",
    "        temperature (float): Temperature parameter to adjust randomness in character sampling.\n",
    "        \n",
    "    Returns:\n",
    "        generated_name (str): The generated entity name.\n",
    "    \"\"\"\n",
    "    invalid_chars = [\"'\", \"-\", \" \"]  # Characters that should not appear at the start or end.\n",
    "\n",
    "    # Initialize the seed with a random character:\n",
    "    start_idx = random.randint(0, len(char_to_idx) - 1)\n",
    "    seed = idx_to_char[start_idx]\n",
    "    \n",
    "    # Ensure the name doesn't start with an invalid character:\n",
    "    while seed in invalid_chars:\n",
    "        start_idx = random.randint(0, len(char_to_idx) - 1)\n",
    "        seed = idx_to_char[start_idx]\n",
    "\n",
    "    encoded_seed = [start_idx]  # Start seed as an index.\n",
    "    generated_name = seed  # Initialize the generated name with the seed character.\n",
    "    \n",
    "    # Fill seed if it's too short:\n",
    "    while len(encoded_seed) < 5:  # Ensure seed length is at least seq_length:\n",
    "        encoded_seed.append(random.randint(0, len(char_to_idx) - 1))\n",
    "\n",
    "    # Generate characters:\n",
    "    for _ in range(length):\n",
    "        input_seq = np.array(encoded_seed[-5:]).reshape(1, 5, 1) / len(char_to_idx)\n",
    "        predicted_prob = model.predict(input_seq, verbose=0)[0]\n",
    "\n",
    "        # Apply temperature to predictions:\n",
    "        predicted_prob = np.log(predicted_prob + 1e-7) / temperature\n",
    "        predicted_prob = np.exp(predicted_prob) / np.sum(np.exp(predicted_prob))\n",
    "\n",
    "        predicted_char_idx = np.random.choice(len(predicted_prob), p=predicted_prob)\n",
    "        predicted_char = idx_to_char[predicted_char_idx]\n",
    "        \n",
    "        generated_name += predicted_char\n",
    "        encoded_seed.append(predicted_char_idx)\n",
    "\n",
    "    # Post-process to remove consecutive duplicates:\n",
    "    def remove_consecutive_duplicates(name):\n",
    "        result = [name[0]]\n",
    "        for char in name[1:]:\n",
    "            if result[-1] != char:\n",
    "                result.append(char)\n",
    "        return ''.join(result)\n",
    "\n",
    "    generated_name = remove_consecutive_duplicates(generated_name).strip()\n",
    "    generated_name = generated_name.capitalize()\n",
    "\n",
    "    # Validate apostrophe placement:\n",
    "    if \"'\" in generated_name:\n",
    "        parts = generated_name.split(\"'\")\n",
    "        if len(parts) > 2 or parts[0] == \"\" or parts[-1] == \"\":\n",
    "            return generate_entity_name(length, temperature)\n",
    "        parts[1] = parts[1].lower()\n",
    "        generated_name = \"'\".join(parts)\n",
    "\n",
    "    # Validate that name doesn't end with invalid characters:\n",
    "    if generated_name[-1] in invalid_chars:\n",
    "        return generate_entity_name(length, temperature)\n",
    "\n",
    "    return generated_name\n",
    "\n",
    "# Generate and display a sample name:\n",
    "new_entity = generate_entity_name(length=10, temperature=0.7)\n",
    "print(f\"Sample Name: {new_entity}\")\n",
    "\n",
    "# Save trained model:\n",
    "model.save('final_model.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define possible attributes for to-be generated entities:\n",
    "races = [\"Elder God\", \"Shoggoth\", \"Night Gaunt\", \"Deep One\", \"Great Old One\", \"Cosmic Horror\"]\n",
    "powers = [\"Mighty\", \"Weak\", \"Ancient\", \"Vast\", \"Primordial\", \"Frail\"]\n",
    "domains = [\"Cosmic Abyss\", \"The Void\", \"Earth's Oceans\", \"Dream Realms\", \"The Stars\", \"Dark Cosmos\"]\n",
    "physical_traits = [\"Amorphous\", \"Tentacled\", \"Winged\", \"Eyeless\", \"Eyes of Madness\", \"Unseen\"]\n",
    "alignments = [\"Malevolent\", \"Neutral\", \"Indifferent\", \"Benevolent\"]\n",
    "\n",
    "# --- Step 1: Generate Random Set Of Attributes ---\n",
    "def generate_entity_attributes():\n",
    "    race = random.choice(races)  # Race.\n",
    "    power = random.choice(powers)  # Power.\n",
    "    domain = random.choice(domains)  # Domain.\n",
    "    traits = random.sample(physical_traits, 2)  # Phyiscal traits x 2.\n",
    "    alignment = random.choice(alignments)  # Alignment.\n",
    "    \n",
    "    return {\n",
    "        \"race\": race,\n",
    "        \"power\": power,\n",
    "        \"domain\": domain,\n",
    "        \"physical_traits\": traits,\n",
    "        \"alignment\": alignment\n",
    "    }\n",
    "\n",
    "# --- Step 2: Generate Entity Name With Random Attributes ---\n",
    "def generate_entity_name_with_attributes(length=10, temperature=1.0):\n",
    "    generated_name = generate_entity_name(length=length, temperature=temperature)\n",
    "    attributes = generate_entity_attributes()\n",
    "    \n",
    "    # Bundle name and attributes into a dictionary:\n",
    "    entity_details = {\n",
    "        \"name\": generated_name,\n",
    "        \"race\": attributes[\"race\"],\n",
    "        \"power\": attributes[\"power\"],\n",
    "        \"domain\": attributes[\"domain\"],\n",
    "        \"physical_traits\": ', '.join(attributes[\"physical_traits\"]),  # Convert traits list to a string\n",
    "        \"alignment\": attributes[\"alignment\"]\n",
    "    }\n",
    "    \n",
    "    return entity_details\n",
    "\n",
    "# --- Step 3: Generate Entity And Print Its Details ---\n",
    "new_entity = generate_entity_name_with_attributes(length=10, temperature=1)\n",
    "print(f\"New Creature: {new_entity['name']}\")\n",
    "print(f\"Race: {new_entity['race']}\")\n",
    "print(f\"Power: {new_entity['power']}\")\n",
    "print(f\"Domain: {new_entity['domain']}\")\n",
    "print(f\"Physical Traits: {new_entity['physical_traits']}\")\n",
    "print(f\"Alignment: {new_entity['alignment']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
