{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "\n",
    "# Set up necessary directories and configurations\n",
    "os.makedirs('data', exist_ok=True)\n",
    "session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to clean title by standardizing the \"By H. P. Lovecraft\" text\n",
    "def clean_title(title):\n",
    "    author_text = \"By H. P. Lovecraft\"\n",
    "    # Remove duplicate \"By H. P. Lovecraft\" and standardize spacing\n",
    "    title = re.sub(rf\"({author_text}\\s*)+\", author_text, title).strip()\n",
    "    if title.endswith(author_text) and not title.endswith(\" \" + author_text):\n",
    "        title = title.replace(author_text, \" \" + author_text)\n",
    "    return title\n",
    "\n",
    "# --- Step 1: Scraping Lovecraft Works ---\n",
    "\n",
    "def scrape_lovecraft_content(content_type):\n",
    "    base_url = \"https://www.hplovecraft.com/writings/texts/\"\n",
    "    response = session.get(base_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access the base URL: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content_links = [\n",
    "        f\"{base_url}{link['href']}\"\n",
    "        for link in soup.find_all('a', href=True)\n",
    "        if link['href'].startswith(f'{content_type}/') and not link['href'].startswith('#')\n",
    "    ]\n",
    "\n",
    "    csv_filename = f'data/lovecraft_{content_type}.csv'\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Content Type', 'Title', 'Text'])\n",
    "\n",
    "        for content_url in content_links:\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            try:\n",
    "                content_response = session.get(content_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                if content_response.status_code == 200:\n",
    "                    content_soup = BeautifulSoup(content_response.content, 'html.parser')\n",
    "                    title_tag = content_soup.find('font', size=\"+2\")\n",
    "                    author_tag = content_soup.find('font', size=\"+1\")\n",
    "                    text_div = content_soup.find('div', align='justify')\n",
    "\n",
    "                    if title_tag and text_div:\n",
    "                        title = f\"{title_tag.get_text(strip=True)} by {author_tag.get_text(strip=True)}\"\n",
    "                        title = clean_title(title)  # Clean the title text\n",
    "                        csvwriter.writerow([content_type, title, text_div.get_text(strip=True)])\n",
    "                        print(f'Scraped: {title}')\n",
    "                    else:\n",
    "                        print(f'Title or text not found for {content_url}')\n",
    "                else:\n",
    "                    print(f'Failed to scrape {content_url}: {content_response.status_code}')\n",
    "            except Exception as e:\n",
    "                print(f'Error scraping {content_url}: {e}')\n",
    "\n",
    "# Scrape for all specified content types\n",
    "for content in ['fiction', 'poetry', 'essays', 'letters']:\n",
    "    scrape_lovecraft_content(content)\n",
    "\n",
    "# --- Step 2: API Data Collection ---\n",
    "\n",
    "def fetch_and_save_json(api_url, filename):\n",
    "    response = session.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(f'data/{filename}.json', 'w', encoding='utf-8') as file:\n",
    "            json.dump(response.json(), file, ensure_ascii=False, indent=4)\n",
    "        print(f\"{filename.capitalize()} data saved successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {filename} data: {response.status_code}\")\n",
    "\n",
    "api_categories = {\n",
    "    \"creatures\": \"https://lovecraftapirest.fly.dev/api/creatures\",\n",
    "    \"races\": \"https://lovecraftapirest.fly.dev/api/races\",\n",
    "    \"outer_gods\": \"https://lovecraftapirest.fly.dev/api/categories/outer-gods\",\n",
    "    \"great_old_ones\": \"https://lovecraftapirest.fly.dev/api/categories/great-old-ones\",\n",
    "    \"lesser_old_ones\": \"https://lovecraftapirest.fly.dev/api/categories/lesser-old-ones\"\n",
    "}\n",
    "\n",
    "for name, url in api_categories.items():\n",
    "    fetch_and_save_json(url, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Step 3: Load and Clean Data ---\n",
    "# Load JSON files\n",
    "with open('data/creatures.json') as f:\n",
    "    creatures = json.load(f)\n",
    "\n",
    "with open('data/great_old_ones.json') as f:\n",
    "    great_old_ones = json.load(f)\n",
    "\n",
    "with open('data/lesser_old_ones.json') as f:\n",
    "    lesser_old_ones = json.load(f)\n",
    "\n",
    "with open('data/outer_gods.json') as f:\n",
    "    outer_gods = json.load(f)\n",
    "\n",
    "with open('data/races.json') as f:\n",
    "    races = json.load(f)\n",
    "\n",
    "# Load CSV file with Lovecraft fiction\n",
    "lovecraft_data = pd.read_csv('data/lovecraft_fiction.csv')\n",
    "\n",
    "# Combine all entity names and add a 'Type' for each entity\n",
    "all_entities = []\n",
    "\n",
    "# Add creatures with type 'Creature'\n",
    "all_entities.extend([{'name': creature['name'], 'type': 'Creature'} for creature in creatures])\n",
    "\n",
    "# Add Great Old Ones with type 'Great Old One'\n",
    "all_entities.extend([{'name': old_one['name'], 'type': 'Great Old One'} for old_one in great_old_ones])\n",
    "\n",
    "# Add Lesser Old Ones with type 'Lesser Old One'\n",
    "all_entities.extend([{'name': old_one['name'], 'type': 'Lesser Old One'} for old_one in lesser_old_ones])\n",
    "\n",
    "# Add Outer Gods with type 'Outer God'\n",
    "all_entities.extend([{'name': old_one['name'], 'type': 'Outer God'} for old_one in outer_gods])\n",
    "\n",
    "# Add races with type 'Race'\n",
    "all_entities.extend([{'name': race['race'], 'type': 'Race'} for race in races])\n",
    "\n",
    "# Clean names to lowercase for case-insensitive matching and merge ghoul entities\n",
    "all_names = [entity['name'].lower() for entity in all_entities]\n",
    "all_types = [entity['type'] for entity in all_entities]\n",
    "\n",
    "# Merge plural forms and combine duplicates\n",
    "def clean_name(name):\n",
    "    name_corrections = {\n",
    "        'ghoul': 'ghoul',\n",
    "        'dark young': 'dark young of shub-niggurath',\n",
    "        'deep one': 'deep one',\n",
    "        'dimensional shambler': 'dimensional shambler',\n",
    "        'elder thing': 'elder thing',\n",
    "        'flying polyp': 'flying polyp',\n",
    "        'ghast': 'ghast',\n",
    "        'gug': 'gug',\n",
    "        'hound of tindalos': 'hound of tindalos',\n",
    "        'moon-beast': 'moon-beast',\n",
    "        'night-gaunt': 'night-gaunt',\n",
    "        'shoggoth': 'shoggoth',\n",
    "        'spider of leng': 'spider of leng',\n",
    "        'star spawn of cthulhu': 'star spawn of cthulhu',\n",
    "        'tcho-tcho': 'tcho-tcho'\n",
    "    }\n",
    "    # Clean the name to ensure it matches the corrected form\n",
    "    for singular, corrected in name_corrections.items():\n",
    "        if singular in name:\n",
    "            return corrected\n",
    "    return name\n",
    "\n",
    "# Apply the cleaning function to the list of names\n",
    "all_names = [clean_name(name) for name in all_names]\n",
    "\n",
    "# Drop unwanted terms: 'han', 'ghoul', and 'darkness'\n",
    "unwanted_terms = ['han', 'ghoul', 'darkness']\n",
    "all_names = [name for name in all_names if name not in unwanted_terms]\n",
    "\n",
    "# --- Step 4: Filter Texts by Names and Count Occurrences ---\n",
    "def filter_texts(data_frame, names):\n",
    "    return [text for text in data_frame['Text'] if any(name in text.lower() for name in names)]\n",
    "\n",
    "filtered_texts = filter_texts(lovecraft_data, all_names)\n",
    "\n",
    "# Count occurrences with merged ghoul entities and cleaned names\n",
    "name_counts = Counter()\n",
    "for text in filtered_texts:\n",
    "    for name in all_names:\n",
    "        name_counts[name] += text.lower().count(name)\n",
    "\n",
    "# Create a DataFrame for name counts\n",
    "name_counts_df = pd.DataFrame(name_counts.items(), columns=['Name', 'Count'])\n",
    "\n",
    "# Add the 'Type' column based on name matching\n",
    "name_counts_df['Type'] = name_counts_df['Name'].apply(\n",
    "    lambda name: all_types[all_names.index(name)] if name in all_names else 'Unknown'\n",
    ")\n",
    "\n",
    "# Save the name counts with types\n",
    "name_counts_df.to_csv('data/lovecraft_name_counts.csv', index=False)\n",
    "\n",
    "# --- Step 5: Visualization ---\n",
    "def plot_top_entities(counts_df, top_n=10, exclude=None):\n",
    "    exclude = exclude or []\n",
    "    filtered_df = counts_df[~counts_df['Name'].isin(exclude)]\n",
    "    top_entities = filtered_df.nlargest(top_n, 'Count')\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(top_entities['Name'], top_entities['Count'], color='purple')\n",
    "    plt.title('Top Lovecraftian Entities by Count')\n",
    "    plt.xlabel('Entity Names')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load updated name counts if needed and plot\n",
    "plot_top_entities(name_counts_df, top_n=20, exclude=['darkness'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV:\n",
    "name_counts_df = pd.read_csv('data/lovecraft_name_counts.csv')\n",
    "\n",
    "# Show the top 10 most common entities\n",
    "top_entities = name_counts_df.sort_values(by='Count', ascending=False).head(10)\n",
    "print(top_entities)\n",
    "\n",
    "# Show the least common entities\n",
    "least_common_entities = name_counts_df.sort_values(by='Count', ascending=True).head(10)\n",
    "print(least_common_entities)\n",
    "\n",
    "# You can also filter by entity type (if it's included in your dataset)\n",
    "# For example, if you have a 'Type' column:\n",
    "# lovecraft_data_filtered = name_counts_df[name_counts_df['Type'] == 'Great Old One']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Load CSV file:\n",
    "df = pd.read_csv('data/lovecraft_name_counts.csv')\n",
    "\n",
    "names = df['Name'].tolist()\n",
    "\n",
    "# Clean up and prepare 'Name' for character-level tokenization:\n",
    "names = [name.replace(\" \", \"\") for name in names]  # Remove spaces.\n",
    "all_chars = ''.join(names)\n",
    "vocab = sorted(set(all_chars))  # Unique characters.\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}  # Mapping chars to indices.\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}  # Reverse mapping.\n",
    "\n",
    "# Generate input-output sequences:\n",
    "seq_length = 5  # Length of character sequences used to predict next character:\n",
    "X = []\n",
    "y = []\n",
    "for name in names:\n",
    "    for i in range(len(name) - seq_length):\n",
    "        X.append([char_to_idx[char] for char in name[i:i + seq_length]])\n",
    "        y.append(char_to_idx[name[i + seq_length]])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Reshape X for LSTM model input (samples, time steps, features):\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1)) / len(vocab)  # Normalize the input.\n",
    "\n",
    "# One-hot encode the labels:\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(vocab))\n",
    "\n",
    "# Define the LSTM model with more layers and units:\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))  # Dropout layer to prevent overfitting.\n",
    "model.add(LSTM(128, return_sequences=False))  # Second LSTM layer.\n",
    "model.add(Dense(len(vocab), activation='softmax'))  # Output layer.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Custom callback to track highest accuracy\n",
    "class HighestAccuracyCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.highest_accuracy = 0.0\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_accuracy = logs.get(\"accuracy\")\n",
    "        if current_accuracy and current_accuracy > self.highest_accuracy:\n",
    "            self.highest_accuracy = current_accuracy\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(f\"Highest accuracy achieved: {self.highest_accuracy:.4f}\")\n",
    "\n",
    "# Instantiate the callback\n",
    "highest_accuracy_callback = HighestAccuracyCallback()\n",
    "\n",
    "# Train model with the custom callback\n",
    "model.fit(X, y, epochs=1000, batch_size=64, callbacks=[highest_accuracy_callback])\n",
    "\n",
    "# Define function to generate entity names (same as provided in your code)\n",
    "def generate_entity_name(length=10, temperature=1.0):\n",
    "    # Define invalid starting and ending characters:\n",
    "    invalid_chars = [\"'\", \"-\", \" \"]\n",
    "\n",
    "    # Start from a random character in the vocabulary:\n",
    "    start_idx = random.randint(0, len(vocab) - 1)\n",
    "    seed = idx_to_char[start_idx]  # Start the name from this random character.\n",
    "    \n",
    "    # Ensure name does not start with invalid character:\n",
    "    while seed in invalid_chars:\n",
    "        start_idx = random.randint(0, len(vocab) - 1)\n",
    "        seed = idx_to_char[start_idx]\n",
    "\n",
    "    # Initialize seed as a list of indices:\n",
    "    encoded_seed = [start_idx]  # Convert character to index.\n",
    "\n",
    "    generated_name = seed  # Initialize generated name with the first character.\n",
    "    \n",
    "    # Ensure there are enough characters in encoded_seed for model:\n",
    "    while len(encoded_seed) < seq_length:\n",
    "        # Add a random character to the seed if it's too short.\n",
    "        encoded_seed.append(random.randint(0, len(vocab) - 1))\n",
    "    \n",
    "    # Generate characters one at a time:\n",
    "    for _ in range(length):\n",
    "        input_seq = np.array(encoded_seed[-seq_length:]).reshape(1, seq_length, 1) / len(vocab)  # Prepare input for LSTM.\n",
    "        predicted_prob = model.predict(input_seq, verbose=0)[0]  # Get prediction probabilities.\n",
    "\n",
    "        # Apply temperature to the probabilities:\n",
    "        predicted_prob = np.asarray(predicted_prob).astype('float64')\n",
    "        predicted_prob = np.log(predicted_prob + 1e-7) / temperature  # Add small value to avoid log(0).\n",
    "        predicted_prob = np.exp(predicted_prob) / np.sum(np.exp(predicted_prob))  # Normalize probabilities.\n",
    "\n",
    "        # Sample next character:\n",
    "        predicted_char_idx = np.random.choice(len(predicted_prob), p=predicted_prob)\n",
    "        predicted_char = idx_to_char[predicted_char_idx]\n",
    "        \n",
    "        generated_name += predicted_char\n",
    "        encoded_seed.append(predicted_char_idx)  # Update seed with predicted character.\n",
    "\n",
    "    # Post-process to remove consecutive duplicates:\n",
    "    def remove_consecutive_duplicates(name):\n",
    "        result = []\n",
    "        for char in name:\n",
    "            if not result or result[-1] != char:\n",
    "                result.append(char)\n",
    "        return ''.join(result)\n",
    "\n",
    "    generated_name = remove_consecutive_duplicates(generated_name).strip()\n",
    "\n",
    "    # Capitalize first letter, and lowercase rest of name (after apostrophe if any):\n",
    "    generated_name = generated_name.capitalize()  # Capitalize the first letter of the name.\n",
    "\n",
    "    # Ensure apostrophes are used only in the middle of the name:\n",
    "    if \"'\" in generated_name:\n",
    "        parts = generated_name.split(\"'\")\n",
    "        if len(parts) > 2 or parts[0] == \"\" or parts[-1] == \"\":  # Avoid multiple apostrophes or apostrophes at start/end.\n",
    "            return generate_entity_name(length, temperature)  # Recursively regenerate if apostrophe misuse detected.\n",
    "        parts[1] = parts[1].lower()  # Lowercase after apostrophe.\n",
    "        generated_name = \"'\".join(parts)\n",
    "\n",
    "    # Ensure generated name doesn't end with an invalid character:\n",
    "    if generated_name[-1] in invalid_chars:\n",
    "        return generate_entity_name(length, temperature)  # Regenerate if it ends with invalid char.\n",
    "\n",
    "    return generated_name\n",
    "\n",
    "# Generate Sample Name:\n",
    "new_entity = generate_entity_name(length=10, temperature=0.7)\n",
    "print(f\"Sample Name: {new_entity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 'final' model:\n",
    "model.save('final_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes for entities:\n",
    "races = [\"Elder God\", \"Shoggoth\", \"Night Gaunt\", \"Deep One\", \"Great Old One\", \"Cosmic Horror\"]\n",
    "powers = [\"Mighty\", \"Weak\", \"Ancient\", \"Vast\", \"Primordial\", \"Frail\"]\n",
    "domains = [\"Cosmic Abyss\", \"The Void\", \"Earth's Oceans\", \"Dream Realms\", \"The Stars\", \"Dark Cosmos\"]\n",
    "physical_traits = [\"Amorphous\", \"Tentacled\", \"Winged\", \"Eyeless\", \"Eyes of Madness\", \"Unseen\"]\n",
    "alignments = [\"Malevolent\", \"Neutral\", \"Indifferent\", \"Benevolent\"]\n",
    "\n",
    "# Generate a random set of attributes:\n",
    "def generate_entity_attributes():\n",
    "    race = random.choice(races)\n",
    "    power = random.choice(powers)\n",
    "    domain = random.choice(domains)\n",
    "    traits = random.sample(physical_traits, 2)\n",
    "    alignment = random.choice(alignments)\n",
    "    \n",
    "    return {\n",
    "        \"race\": race,\n",
    "        \"power\": power,\n",
    "        \"domain\": domain,\n",
    "        \"physical_traits\": traits,\n",
    "        \"alignment\": alignment\n",
    "    }\n",
    "\n",
    "def generate_entity_name_with_attributes(length=10, temperature=1.0):\n",
    "    generated_name = generate_entity_name(length=length, temperature=temperature)\n",
    "\n",
    "    attributes = generate_entity_attributes()\n",
    "    \n",
    "    entity_details = {\n",
    "        \"name\": generated_name,\n",
    "        \"race\": attributes[\"race\"],\n",
    "        \"power\": attributes[\"power\"],\n",
    "        \"domain\": attributes[\"domain\"],\n",
    "        \"physical_traits\": ', '.join(attributes[\"physical_traits\"]),\n",
    "        \"alignment\": attributes[\"alignment\"]\n",
    "    }\n",
    "    \n",
    "    return entity_details\n",
    "\n",
    "# Generate new entity with name and attributes:\n",
    "new_entity = generate_entity_name_with_attributes(length=10, temperature=1)\n",
    "print(f\"New Creature: {new_entity['name']}\")\n",
    "print(f\"Race: {new_entity['race']}\")\n",
    "print(f\"Power: {new_entity['power']}\")\n",
    "print(f\"Domain: {new_entity['domain']}\")\n",
    "print(f\"Physical Traits: {new_entity['physical_traits']}\")\n",
    "print(f\"Alignment: {new_entity['alignment']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
