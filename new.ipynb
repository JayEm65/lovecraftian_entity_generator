{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marc Jay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TQDMNotebookCallback' from 'tqdm.keras' (c:\\Users\\Marc Jay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\keras.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TQDMNotebookCallback  \u001b[38;5;66;03m# Add this for tqdm progress bar\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load JSON files\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(file_path):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TQDMNotebookCallback' from 'tqdm.keras' (c:\\Users\\Marc Jay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\keras.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm.keras import TQDMNotebookCallback  # Add this for tqdm progress bar\n",
    "\n",
    "# Load JSON files\n",
    "def load_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "creatures = load_json('data/creatures.json')\n",
    "great_old_ones = load_json('data/great_old_ones.json')\n",
    "lesser_old_ones = load_json('data/lesser_old_ones.json')\n",
    "outer_gods = load_json('data/outer_gods.json')\n",
    "races = load_json('data/races.json')\n",
    "\n",
    "# Extract names from JSON\n",
    "def extract_names(data):\n",
    "    return [item['name'] for item in data]\n",
    "\n",
    "json_names = (\n",
    "    extract_names(creatures) +\n",
    "    extract_names(great_old_ones) +\n",
    "    extract_names(lesser_old_ones) +\n",
    "    extract_names(outer_gods) +\n",
    "    [race['race'] for race in races]\n",
    ")\n",
    "\n",
    "# Load Lovecraft fiction data\n",
    "lovecraft_data = pd.read_csv('data/lovecraft_fiction.csv')\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\']', '', text)  # Allow apostrophes\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'Text' column\n",
    "lovecraft_data['Cleaned_Text'] = lovecraft_data['Text'].apply(clean_text)\n",
    "\n",
    "# Filter texts based on names\n",
    "def filter_texts(data_frame, names):\n",
    "    return [\n",
    "        text for text in data_frame['Text']\n",
    "        if any(name.lower() in text.lower() for name in names)\n",
    "    ]\n",
    "\n",
    "filtered_texts = filter_texts(lovecraft_data, json_names)\n",
    "\n",
    "# Count occurrences of names in filtered texts\n",
    "name_counts = Counter()\n",
    "for text in filtered_texts:\n",
    "    for name in json_names:\n",
    "        name_counts[name] += text.lower().count(name.lower())\n",
    "\n",
    "# Create DataFrame for name counts\n",
    "name_counts_df = pd.DataFrame(name_counts.items(), columns=['Name', 'Count'])\n",
    "\n",
    "# Plot top names\n",
    "def plot_top_names(name_counts_df, top_n=20):\n",
    "    top_names = name_counts_df.nlargest(top_n, 'Count')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(top_names['Name'], top_names['Count'], color='skyblue')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Top Names in Lovecraftâ€™s Works')\n",
    "    plt.xlabel('Name')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top_names(name_counts_df)\n",
    "\n",
    "# Save the name counts\n",
    "name_counts_df.to_csv('data_processed/lovecraft_name_counts.csv', index=False)\n",
    "\n",
    "# Tokenize descriptions and count tokens\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_descriptions(lovecraft_data):\n",
    "    entities = []\n",
    "    for _, row in lovecraft_data.iterrows():\n",
    "        title = row['Title'].lower()\n",
    "        text = row['Cleaned_Text']\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        sentences = sent_tokenize(text)\n",
    "        phrases = [phrase for phrase in sentences if len(phrase.split()) > 1]\n",
    "        entities.append({'name': title, 'description': text, 'tokens': tokens, 'phrases': phrases})\n",
    "    return pd.DataFrame(entities)\n",
    "\n",
    "processed_df = tokenize_descriptions(lovecraft_data)\n",
    "processed_df.to_csv('data_processed/lovecraft_processed_entities.csv', index=False)\n",
    "\n",
    "# Count token frequencies\n",
    "all_tokens = processed_df['tokens'].explode().tolist()\n",
    "token_counts = Counter(all_tokens)\n",
    "token_counts_df = pd.DataFrame(token_counts.items(), columns=['Token', 'Frequency'])\n",
    "token_counts_df.to_csv('data_processed/lovecraft_token_frequencies.csv', index=False)\n",
    "\n",
    "# Extract unique phrases\n",
    "all_phrases = [phrase for phrases in processed_df['phrases'] for phrase in phrases]\n",
    "phrase_counts = Counter(all_phrases)\n",
    "phrase_counts_df = pd.DataFrame(phrase_counts.items(), columns=['Phrase', 'Frequency'])\n",
    "phrase_counts_df.to_csv('data_processed/lovecraft_phrase_frequencies.csv', index=False)\n",
    "\n",
    "# Save final name list combining JSON names with filtered text names\n",
    "combined_names = list(set([name.lower() for name in json_names] + list(token_counts.keys())))\n",
    "with open('data_processed/final_name_list.txt', 'w') as f:\n",
    "    for name in combined_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "# Count occurrences of names\n",
    "name_occurrences = Counter()\n",
    "for name in json_names:\n",
    "    name_occurrences[name] = name_occurrences.get(name, 0) + sum(row['Cleaned_Text'].lower().count(name.lower()) for _, row in lovecraft_data.iterrows())\n",
    "\n",
    "# Count occurrences of phrases\n",
    "phrase_occurrences = Counter()\n",
    "for phrases in processed_df['phrases']:\n",
    "    phrase_occurrences.update(phrases)\n",
    "\n",
    "# Create DataFrames for name counts and phrase counts\n",
    "name_counts_df = pd.DataFrame(name_occurrences.items(), columns=['Name', 'Count'])\n",
    "phrase_counts_df = pd.DataFrame(phrase_occurrences.items(), columns=['Phrase', 'Frequency'])\n",
    "\n",
    "# Get top 50 names and phrases\n",
    "top_names = name_counts_df.nlargest(50, 'Count')\n",
    "top_phrases = phrase_counts_df.nlargest(50, 'Frequency')\n",
    "\n",
    "# Print or save the top names and phrases\n",
    "top_names.to_csv('data_processed/top_50_names.csv', index=False)\n",
    "top_phrases.to_csv('data_processed/top_50_phrases.csv', index=False)\n",
    "\n",
    "# Tokenization for deep learning model\n",
    "tokenizer = Tokenizer(num_words=10000)  # Limit vocab size\n",
    "tokenizer.fit_on_texts(lovecraft_data['Cleaned_Text'].tolist())\n",
    "\n",
    "# Convert texts to sequences and pad them\n",
    "sequences = tokenizer.texts_to_sequences(lovecraft_data['Cleaned_Text'].tolist())\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "\n",
    "# Prepare input (X) and output (y)\n",
    "X = padded_sequences[:, :-1]\n",
    "y = padded_sequences[:, 1:]\n",
    "\n",
    "# Use sparse categorical crossentropy instead of one-hot encoding\n",
    "y_sparse = y  # No need to convert to categorical\n",
    "\n",
    "# Define and train LSTM model with sparse categorical crossentropy\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=X.shape[1]),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    TimeDistributed(Dense(10000, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Add tqdm progress bar callback to monitor training progress\n",
    "model.fit(X, y_sparse, batch_size=64, epochs=20, validation_split=0.2, callbacks=[TQDMNotebookCallback()])\n",
    "\n",
    "# Save the model\n",
    "model.save('lovecraft_lstm_model.h5')\n",
    "\n",
    "print(\"Model training completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
