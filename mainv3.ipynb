{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load JSON files\n",
    "def load_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "creatures = load_json('data/creatures.json')\n",
    "great_old_ones = load_json('data/great_old_ones.json')\n",
    "lesser_old_ones = load_json('data/lesser_old_ones.json')\n",
    "outer_gods = load_json('data/outer_gods.json')\n",
    "races = load_json('data/races.json')\n",
    "\n",
    "# Extract names from JSON\n",
    "def extract_names(data):\n",
    "    return [item['name'] for item in data]\n",
    "\n",
    "json_names = (\n",
    "    extract_names(creatures) +\n",
    "    extract_names(great_old_ones) +\n",
    "    extract_names(lesser_old_ones) +\n",
    "    extract_names(outer_gods) +\n",
    "    [race['race'] for race in races]\n",
    ")\n",
    "\n",
    "# Load and filter Lovecraft data\n",
    "lovecraft_data = pd.read_csv('data/lovecraft_fiction.csv')\n",
    "\n",
    "# Filter texts based on names\n",
    "def filter_texts(data_frame, names):\n",
    "    return [\n",
    "        text for text in data_frame['Text'] \n",
    "        if any(name.lower() in text.lower() for name in names)\n",
    "    ]\n",
    "\n",
    "filtered_texts = filter_texts(lovecraft_data, json_names)\n",
    "\n",
    "# Count occurrences of names in filtered texts\n",
    "name_counts = Counter()\n",
    "for text in filtered_texts:\n",
    "    for name in json_names:\n",
    "        name_counts[name] += text.lower().count(name.lower())\n",
    "\n",
    "# Create DataFrame for name counts\n",
    "name_counts_df = pd.DataFrame(name_counts.items(), columns=['Name', 'Count'])\n",
    "\n",
    "# Visualization of name counts\n",
    "def plot_top_names(name_counts_df, top_n=20):\n",
    "    top_names = name_counts_df.nlargest(top_n, 'Count')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(top_names['Name'], top_names['Count'], color='skyblue')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Top Names in Lovecraftâ€™s Works')\n",
    "    plt.xlabel('Name')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top_names(name_counts_df)\n",
    "\n",
    "# Save the name counts to a new CSV file\n",
    "name_counts_df.to_csv('data_processed/lovecraft_name_counts.csv', index=False)\n",
    "\n",
    "# Clean text example function\n",
    "def clean_text(text):\n",
    "    # Lowercase the text and replace unwanted characters with spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text.lower())  # Replace non-alphanumeric with space\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()  # Remove leading/trailing whitespace\n",
    "\n",
    "\n",
    "\n",
    "# Apply cleaning to the 'Text' column\n",
    "lovecraft_data['Cleaned_Text'] = lovecraft_data['Text'].apply(clean_text)\n",
    "\n",
    "# Update name counts based on cleaned text\n",
    "name_occurrences = lovecraft_data['Cleaned_Text'].str.cat(sep=' ').split()\n",
    "name_count_dict = {name: name_occurrences.count(name.lower()) for name in json_names}\n",
    "name_counts_df['Updated_Count'] = name_counts_df['Name'].str.lower().map(name_count_dict).fillna(0).astype(int)\n",
    "\n",
    "# Save updated counts to the data_processed folder\n",
    "name_counts_df.to_csv('data_processed/updated_lovecraft_name_counts.csv', index=False)\n",
    "\n",
    "# Prepare stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize descriptions and count tokens\n",
    "def tokenize_descriptions(lovecraft_data):\n",
    "    entities = []\n",
    "    for index, row in lovecraft_data.iterrows():\n",
    "        title = row['Title'].lower()\n",
    "        text = clean_text(row['Text'])  # Clean the text\n",
    "        tokens = word_tokenize(text)  # Tokenize the cleaned text\n",
    "        tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords\n",
    "        sentences = sent_tokenize(text)\n",
    "        phrases = [phrase for phrase in sentences if len(phrase.split()) > 1]\n",
    "        entities.append({'name': title, 'description': text, 'tokens': tokens, 'phrases': phrases})\n",
    "    return pd.DataFrame(entities)\n",
    "\n",
    "\n",
    "\n",
    "processed_df = tokenize_descriptions(lovecraft_data)\n",
    "processed_df.to_csv('data_processed/lovecraft_processed_entities.csv', index=False)\n",
    "\n",
    "# Count token frequencies from all tokens, not just phrases\n",
    "all_tokens = processed_df['tokens'].explode().tolist()\n",
    "token_counts = Counter(all_tokens)\n",
    "token_counts_df = pd.DataFrame(token_counts.items(), columns=['Token', 'Frequency'])\n",
    "token_counts_df.to_csv('data_processed/lovecraft_token_frequencies.csv', index=False)\n",
    "\n",
    "# Display the top tokens\n",
    "sorted_tokens = token_counts_df.sort_values(by='Frequency', ascending=False)\n",
    "print(sorted_tokens.head(20))\n",
    "\n",
    "# Extract unique phrases\n",
    "all_phrases = [phrase for phrases in processed_df['phrases'] for phrase in phrases]\n",
    "phrase_counts = Counter(all_phrases)\n",
    "phrase_counts_df = pd.DataFrame(phrase_counts.items(), columns=['Phrase', 'Frequency'])\n",
    "phrase_counts_df.to_csv('data_processed/lovecraft_phrase_frequencies.csv', index=False)\n",
    "\n",
    "# Save final name list combining JSON names with filtered text names\n",
    "combined_names = list(set([name.lower() for name in json_names] + list(token_counts.keys())))\n",
    "with open('data_processed/final_name_list.txt', 'w') as f:\n",
    "    for name in combined_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "print(\"Final name list created and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of names\n",
    "name_occurrences = Counter()\n",
    "for name in json_names:\n",
    "    name_occurrences[name] = name_occurrences.get(name, 0) + sum(row['Cleaned_Text'].lower().count(name.lower()) for _, row in lovecraft_data.iterrows())\n",
    "\n",
    "# Count occurrences of phrases\n",
    "phrase_occurrences = Counter()\n",
    "for phrases in processed_df['phrases']:\n",
    "    phrase_occurrences.update(phrases)\n",
    "\n",
    "# Create DataFrames for name counts and phrase counts\n",
    "name_counts_df = pd.DataFrame(name_occurrences.items(), columns=['Name', 'Count'])\n",
    "phrase_counts_df = pd.DataFrame(phrase_occurrences.items(), columns=['Phrase', 'Frequency'])\n",
    "\n",
    "# Get top 50 names and phrases\n",
    "top_names = name_counts_df.nlargest(50, 'Count')\n",
    "top_phrases = phrase_counts_df.nlargest(50, 'Frequency')\n",
    "\n",
    "# Print or save the top names and phrases\n",
    "print(\"Top 50 Names:\")\n",
    "print(top_names)\n",
    "\n",
    "print(\"\\nTop 50 Phrases:\")\n",
    "print(top_phrases)\n",
    "\n",
    "# Optionally, save them to CSV\n",
    "top_names.to_csv('data_processed/top_50_names.csv', index=False)\n",
    "top_phrases.to_csv('data_processed/top_50_phrases.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "\n",
    "\n",
    "# Load your Lovecraft fiction data\n",
    "lovecraft_data = pd.read_csv('data/lovecraft_fiction.csv')  # Adjust this if your file has a different name\n",
    "texts = lovecraft_data['Text'].tolist()  # Replace 'Text' with the correct column name\n",
    "\n",
    "# Clean the text (define clean_text function if not already done)\n",
    "def clean_text(text):\n",
    "    # Implement your cleaning logic here (lowercasing, removing special characters, etc.)\n",
    "    text = text.lower()\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    return text\n",
    "\n",
    "# Clean the text data\n",
    "cleaned_texts = [clean_text(text) for text in texts]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_texts)\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_texts)\n",
    "\n",
    "# Pad sequences\n",
    "max_length = max(len(seq) for seq in sequences)  # Or set a fixed max length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "\n",
    "# Save padded sequences and tokenizer\n",
    "np.savetxt('data/padded_sequences.csv', padded_sequences, delimiter=',', fmt='%d')\n",
    "with open('data/tokenizer.json', 'w') as f:\n",
    "    json.dump(tokenizer.to_json(), f)\n",
    "\n",
    "print(\"Padded sequences and tokenizer saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "\n",
    "# Load padded sequences and tokenizer\n",
    "padded_sequences = pd.read_csv('data/padded_sequences.csv').values\n",
    "tokenizer_json = json.load(open('data/tokenizer.json'))\n",
    "tokenizer = tokenizer_from_json(tokenizer_json)\n",
    "\n",
    "# Prepare input (X) and output (y)\n",
    "X = padded_sequences[:, :-1]  # All but the last token\n",
    "y = padded_sequences[:, 1:]   # All but the first token\n",
    "\n",
    "# Convert output to categorical\n",
    "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=X.shape[1]))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=64, epochs=20, validation_split=0.2)\n",
    "\n",
    "# Save the model\n",
    "model.save('lovecraft_lstm_model.h5')2\n",
    "\n",
    "print(\"Model training completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
